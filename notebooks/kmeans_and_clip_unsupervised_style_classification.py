# -*- coding: utf-8 -*-
"""KMeans and CLIP unsupervised style classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nss6cACsIQ0-kx2mKlSyfZh4zH2KkxSM

# KMeans and CLIP unsupervised style classification
"""

import tensorflow as tf

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

"""## 1. Get CLIP Embeddings for images"""

# Install CLIP from OpenAI
!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

import os
import torch
import clip
import glob
from PIL import Image
from tqdm import tqdm
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

from google.colab import drive
drive.mount('/content/drive')

# Load saved models to use in future tests
import joblib
import os
import numpy as np

pca = joblib.load("/content/drive/MyDrive/art_dna/models/pca_model.joblib")
kmeans = joblib.load("/content/drive/MyDrive/art_dna/models/kmeans_model.joblib")

# Load CLIP model and preprocessing
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

image_folder = '/content/drive/My Drive/art_dna/raw_data/resized'

save_folder = "/content/drive/MyDrive/art_dna/clip_embeddings"  # Folder to save outputs
os.makedirs(save_folder, exist_ok=True)

# === Load all .jpg images and sort ===
all_images = sorted(glob.glob(os.path.join(image_folder, "*.jpg")))

# === Batch settings ===
batch_size = 1000
num_batches = (len(all_images) + batch_size - 1) // batch_size

# === Process in batches ===
for batch_idx in range(num_batches):
    start = batch_idx * batch_size
    end = min((batch_idx + 1) * batch_size, len(all_images))
    batch_images = all_images[start:end]

    # Define paths to save this batch's results
    embed_path = os.path.join(save_folder, f"clip_embeddings_batch_{batch_idx}.npy")
    names_path = os.path.join(save_folder, f"clip_filenames_batch_{batch_idx}.txt")

    # Skip batch if already processed
    if os.path.exists(embed_path):
        try:
            data = np.load(embed_path)
            if data.shape[0] == batch_size:
                print(f"[Batch {batch_idx}] Already processed. Skipping.")
                continue
        except:
            print(f"[Batch {batch_idx}] File corrupted. Will reprocess.")

    print(f"[Batch {batch_idx}] Processing {len(batch_images)} images...")

    batch_embeddings = []
    batch_filenames = []

    for path in tqdm(batch_images):
        try:
            image = preprocess(Image.open(path).convert("RGB")).unsqueeze(0).to(device)
            with torch.no_grad():
                embedding = model.encode_image(image)
            batch_embeddings.append(embedding.cpu().numpy())
            batch_filenames.append(os.path.basename(path))
        except Exception as e:
            print(f"Failed on {path}: {e}")

    # Save embeddings and filenames
    np.save(embed_path, np.vstack(batch_embeddings))
    with open(names_path, "w") as f:
        for name in batch_filenames:
            f.write(name + "\n")

print("‚úÖ All batches processed and saved.")

#merge all embedding batches into one array

embedding_files = sorted(glob.glob("/content/drive/MyDrive/art_dna/clip_embeddings/clip_embeddings_batch_*.npy"))
filename_files = sorted(glob.glob("/content/drive/MyDrive/art_dna/clip_embeddings/clip_filenames_batch_*.txt"))

all_embeddings = np.vstack([np.load(f) for f in embedding_files])

all_filenames = []
for fname in filename_files:
    with open(fname) as f:
        all_filenames.extend(line.strip() for line in f)

print(f"Combined: {len(all_embeddings)} embeddings, {len(all_filenames)} filenames")

"""## 2. Use KMeans to visualize clusters"""

n_clusters = 18
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(all_embeddings)

# Map image names to their cluster
#image_clusters = list(zip(all_filenames, cluster_labels))
#for name, cluster in image_clusters:
    #print(f"{name} ‚Üí Cluster {cluster}")

#Visualize with PCA

pca = PCA(n_components=2)
pca_embeddings = pca.fit_transform(all_embeddings)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], c=cluster_labels, cmap='tab10')
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.title("CLIP Embeddings clustered via KMeans")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.grid(True)
plt.show()

import joblib

save_folder = "/content/drive/MyDrive/art_dna/models"
os.makedirs(save_folder, exist_ok=True)

# Save PCA and KMeans
#joblib.dump(pca, f"{save_folder}/pca_model_v2.joblib")
#joblib.dump(kmeans, f"{save_folder}/kmeans_model_v2.joblib")

"""## 3. Match Clusters to Text Prompts Using CLIP"""

# Define style prompts
style_prompts = [
    'Abstractionism', 'Art Nouveau', 'Baroque',
    'Byzantine Art', 'Cubism', 'Expressionism',
    'Impressionism', 'Mannerism', 'Muralism',
    'Neoplasticism', 'Pop Art', 'Primitivism',
    'Realism', 'Renaissance', 'Romanticism',
    'Suprematism', 'Surrealism', 'Symbolism'
]

# Tokenize text prompts and compute their CLIP embeddings
text_tokens = clip.tokenize(style_prompts).to(device)
with torch.no_grad():
    text_embeddings = model.encode_text(text_tokens).cpu().numpy()

# Save text prompts and embeddings
np.save(os.path.join(save_folder, "style_text_embeddings.npy"), text_embeddings)

with open(os.path.join(save_folder, "style_prompts.txt"), "w") as f:
    for prompt in style_prompts:
        f.write(prompt + "\n")

print("‚úÖ Saved style prompts and embeddings to Drive.")

# Load text embeddings from Drive

text_embeddings_path = os.path.join(save_folder, "style_text_embeddings.npy")
text_embeddings = np.load(text_embeddings_path)

print(f"‚úÖ Loaded text embeddings from {text_embeddings_path}")

from scipy.optimize import linear_sum_assignment

for i, center in enumerate(kmeans.cluster_centers_):
    similarities = np.dot(text_embeddings, center.T)

    # Step 1: Compute similarity matrix (18 styles √ó 18 clusters)
    style_prompt_count = len(style_prompts)
    cluster_count = len(kmeans.cluster_centers_)

    # Similarities: shape (styles, clusters)
    similarity_matrix = np.zeros((style_prompt_count, cluster_count))

    for i, style_emb in enumerate(text_embeddings):
        for j, center in enumerate(kmeans.cluster_centers_):
            similarity_matrix[i, j] = np.dot(style_emb, center)

    # Convert to cost matrix (negative because we want to maximize similarity)
    cost_matrix = -similarity_matrix

    # Solve linear assignment problem
    style_indices, cluster_indices = linear_sum_assignment(cost_matrix)

    # Build mapping
    cluster_to_style = {}
    for s_idx, c_idx in zip(style_indices, cluster_indices):
        style = style_prompts[s_idx]
        score = similarity_matrix[s_idx, c_idx]
        cluster_to_style[c_idx] = (style, score)

# Print results

print("üé® Cluster ‚Üí Style assignments:\n")

for i in range(kmeans.n_clusters):
    if i in cluster_to_style:
        style, score = cluster_to_style[i]
        print(f"Cluster {i} ‚Üí {style} (Score: {score:.2f})")
    else:
        print(f"Cluster {i} ‚Üí Not assigned (ambiguous or extra cluster)")

"""## 4. Upload an image and output the top 5 most similar styles with their similarity scores

"""

from sklearn.metrics.pairwise import cosine_similarity
from google.colab import files

# --- Upload and display the image ---
uploaded = files.upload()
image_path = list(uploaded.keys())[0]

img = Image.open(image_path).convert("RGB")
plt.imshow(img)
plt.axis("off")
plt.title("Uploaded Image")
plt.show()

# --- Extract CLIP embedding ---
image_tensor = preprocess(img).unsqueeze(0).to(device)
with torch.no_grad():
    image_embedding = model.encode_image(image_tensor).cpu().numpy()

# --- Project using the same PCA ---
# This projects the uploaded image embedding into the same PCA space used for visualization.
image_pca = pca.transform(image_embedding)

# --- Predict cluster using KMeans ---
# Predict the cluster of the uploaded image using the KMeans model.
# KMeans prediction should be done on the original embedding space, not the PCA-reduced space.
pred_cluster = kmeans.predict(image_embedding)[0]
print(f"\nüìå Predicted cluster: {pred_cluster}")

# --- Compare cluster center to style prompts ---
cluster_center = kmeans.cluster_centers_[pred_cluster]
similarities = np.dot(text_embeddings, cluster_center.T)
top5_idx = similarities.argsort()[::-1][:5]

print("\nüé® Top 5 matched styles for this cluster:")
for i in top5_idx:
    print(f"- {style_prompts[i]} (Score: {similarities[i]:.2f})")

"""## 5. Visualize nearest images in the same cluster"""

# Load images embeddings from Drive
save_folder = "/content/drive/MyDrive/art_dna/clip_embeddings"
all_embeddings_path = os.path.join(save_folder, "all_clip_embeddings.npy")
all_embeddings = np.load(all_embeddings_path)

print(f"‚úÖ Loaded all images embeddings from {all_embeddings_path}")

#import joblib
import os
import numpy as np

# Define the path to the saved PCA model
save_folder = "/content/drive/MyDrive/art_dna/models"
pca_model_path = os.path.join(save_folder, "pca_model.joblib")
pca_embeddings_save_path = os.path.join(save_folder, "pca_embeddings.npy")

pca = joblib.load(pca_model_path)
pca_embeddings = pca.transform(all_embeddings)

# Save the PCA embeddings
np.save(pca_embeddings_save_path, pca_embeddings)

filename_files = sorted(glob.glob("/content/drive/MyDrive/art_dna/clip_embeddings/clip_filenames_batch_*.txt"))
all_filenames = []
for fname in filename_files:
    with open(fname) as f:
        all_filenames.extend(line.strip() for line in f)

# --- Get indices of all images in the same cluster ---
same_cluster_indices = np.where(kmeans.labels_ == pred_cluster)[0]

# --- Compute cosine similarity to uploaded image ---
cluster_embeddings = pca_embeddings[same_cluster_indices]  # PCA-reduced image embeddings
sims = cosine_similarity(image_pca, cluster_embeddings)[0]  # Shape: (n_cluster_images,)

# --- Get top 5 most similar images ---
top_indices = sims.argsort()[::-1][:5]
top_image_indices = same_cluster_indices[top_indices]
top_image_names = [all_filenames[i] for i in top_image_indices]

# --- Plot the similar images ---
fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for ax, img_path in zip(axes, top_image_names):
    img = Image.open(os.path.join(image_folder, img_path))
    ax.imshow(img)
    ax.axis("off")
    ax.set_title(os.path.basename(img_path), fontsize=8)
plt.suptitle("üñºÔ∏è Most Similar Images in Same Cluster")
plt.tight_layout()
plt.show()

"""## 6. Plot similar images with overlaying predicted style label for each retrieved image"""

# --- Plot the similar images with predicted style label ---
fig, axes = plt.subplots(1, 5, figsize=(15, 3))

for ax, img_idx in zip(axes, top_image_indices):
    img_path = all_filenames[img_idx]
    img = Image.open(os.path.join(image_folder, img_path))

    predicted_style = cluster_to_style[kmeans.labels_[img_idx]]

    ax.imshow(img)
    ax.axis("off")
    ax.set_title(predicted_style, fontsize=8)

plt.suptitle("üñºÔ∏è Most Similar Images with Predicted Styles")
plt.tight_layout()
plt.show()

"""## Saving all clip embeddingscombined and pca embeddings on Drive"""

#merge all embedding batches into one array

embedding_files = sorted(glob.glob("/content/drive/MyDrive/art_dna/clip_embeddings/clip_embeddings_batch_*.npy"))
filename_files = sorted(glob.glob("/content/drive/MyDrive/art_dna/clip_embeddings/clip_filenames_batch_*.txt"))

all_embeddings = np.vstack([np.load(f) for f in embedding_files])

all_filenames = []
for fname in filename_files:
    with open(fname) as f:
        all_filenames.extend(line.strip() for line in f)

print(f"Combined: {len(all_embeddings)} embeddings, {len(all_filenames)} filenames")

# Save the combined embeddings to Google Drive
save_folder = "/content/drive/MyDrive/art_dna/clip_embeddings"
all_embeddings_save_path = os.path.join(save_folder, "all_clip_embeddings.npy")
np.save(all_embeddings_save_path, all_embeddings)
print(f"‚úÖ Saved combined embeddings to {all_embeddings_save_path}")

